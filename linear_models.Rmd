---
title: "Linear Models"
author: "Linh Tran"
date: "11/21/2020"
output: 
    html_document:
      toc: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d


set.seed(1)

```

# Linear models

Load and cleans the `airbnb` data.

```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") %>% 
  select(price, stars, borough, neighborhood, room_type)

```

## Model fitting

Scatterplot:

```{r}
#price = continuous variable, stars = numeric predictor, borough = categorical predictor

nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price, color = borough)) +
  geom_point()
```

Fit a model consider price as an outcome that may depend on rating and borough

```{r}
fit = lm(price ~ stars + borough, data = nyc_airbnb)
fit
```

You can also specify an intercept-only model (outcome ~ 1), a model with no intercept (outcome ~ 0 + ...), and a model using all available predictors (outcome ~ .)

## Tidying output

The output of a lm is an object of class lm – a very specific list that isn’t a dataframe but that can be manipulated using other functions. Some common functions for interacting with lm fits are below, although we omit the output:

```{r, include = FALSE}
fit
summary(fit)  #produces an object of class summary.lm, which is a list
summary(fit)$coef
coef(fit) #produce a vector of coef values
fitted.values(fit)  #vector of fitted values
residuals(fit)

broom::glance(fit)  #gives me some useful numbers
```

The `broom` package has functions for obtaining a quick summary of the model and for cleaning up the coefficient table. Both of these functions produce df.

```{r}
broom::tidy(fit) %>% #structured as a df
  select(-std.error, -statistic) %>% 
  mutate(
    term = str_replace(term, "borough", "Borough: ")
  ) %>% 
  knitr::kable(digits = 3)
```

## Be in control of factors

R treat categorical (factor) covariates as such: indicator variables are created for each non-reference category and included in model, and the factor level is treated as the reference.

```{r}
nyc_airbnb =
  nyc_airbnb %>% 
  mutate(
    borough = fct_infreq(borough),
    room_type = fct_infreq(room_type)
  )

nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price, color = borough)) +
  geom_point()
```

Refit my model: now Manhattan is the reference 

```{r}
fit = lm(price ~ stars + borough, data = nyc_airbnb)

broom::tidy(fit)
```

## Diagnostics

Regression diagnostics can identify issues in model fit, especially related to certian failures in model assumptions. The `modelr` package can be used to add residuals and fitted values to a dataframe.

```{r}
modelr::add_residuals(data = nyc_airbnb, model = fit)
# First one is $9.47 higher than what my model predicts.
```

Look at distribution of residuals:

```{r}
modelr::add_residuals(data = nyc_airbnb, model = fit) %>% 
  ggplot(aes(x = borough, y = resid)) +
  geom_violin() +
  ylim(-500, 1500)

#Distributions of residuals are skewed. 

nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = stars, y = resid)) +
  geom_point() +
  facet_wrap(. ~ borough)

#Distributions of residual spread out as star counts increases. 
```

For exclusion of outliers: transform the price variable (e.g. using a log transformation), or fitting a model that is robust to outliers. E.g: a combination of median regression, which is less sensitive to outliers than OLS, maybe bootstrapping for inference. 

## Hypothesis testing

* Model summaries include results of t-tests for single coefficients, and are the standard way of assessing statistical significance.
* Testing multiple coef is somewhat more complicated.  A useful approach is to use nested models, meaning that the terms in a simple “null” model are a subset of the terms in a more complex “alternative” model.

This does t-test by default

```{r}
fit %>% 
  broom::tidy()
```

Assess the significance of a categorical predictor with more than two levels 

```{r}
fit_null = lm(price ~ stars, data = nyc_airbnb)
fit_alt = lm(price~stars + borough, data = nyc_airbnb)

anova(fit_null, fit_alt) %>% 
  broom::tidy()
```


## Nest data, fit models

* Fitting models to datasets nested within variables, meaning, that we'll use `nest` to create a list column containing datasets and fit separate models to each. 

In the airbnb data, we might think that star ratings and room type affects price differently in each borough. One way to allow this kind of effect modification is through interaction terms:

```{r}
#interaction

fit = lm(price ~ stars * borough + room_type * borough, data = nyc_airbnb)

broom::tidy(fit)

# another way
nyc_airbnb %>% 
  lm(price ~ stars * borough + room_type * borough, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

Alternatively, we can nest within boroughs and fit borough-specific models associating price with rating and room type:

```{r}
nyc_airbnb %>% 
  nest(data = -borough) %>% 
  mutate(
    models = map(.x = data, ~lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  filter(term != "(Intercept)") %>% 
  select(borough, term, estimate) %>% 
  pivot_wider(
    names_from = borough,
    values_from = estimate
  )
  
```

**Fitting models to nested datasets is a way of performing stratified analyses. These have a tradeoff: stratified models make it easy to interpret covariate effects in each stratum, but don’t provide a mechanism for assessing the significance of differences across strata.**

An even more extreme example is the assessment of neighborhood effects in Manhattan. The code chunk below fits neighborhood-specific models and shows neighborhood-specific estimates for the coefficients related to room type:

```{r}
nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  nest(data = -neighborhood) %>% 
  mutate(
    models = map(.x = data, ~lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(-data, -models) %>% 
  unnest(results)  %>% 
  filter(str_detect(term, "room_type")) %>% 
  ggplot(aes(x = neighborhood, y = estimate)) +
  geom_point() +
  facet_wrap(. ~ term) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

#There is, generally speaking, a reduction in room price for a private room or a shared room compared to an entire apartment, but this varies quite a bit across neighborhoods.
```

Code to fit a mixed model with neighborhood-level random intercepts and random slopes for each room type

```{r}
manhattan_airbnb =
  nyc_airbnb %>% 
  filter(borough == "Manhattan")

manhattan_airbnb %>% 
  lme4::lmer(price ~ stars + room_type + (1 + room_type | neighborhood), data = .) %>% 
  broom.mixed::tidy()
```

## Binary outcomes

Logistic regression is useful for binary outcomes

```{r, eval = FALSE}
baltimore_df = 
  read_csv("data/homicide-data.csv") %>% 
  filter(city == "Baltimore") %>% 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) %>% 
  select(resolved, victim_age, victim_race, victim_sex)
```

Using these data, we can fit a logistic regression for the binary “resolved” outcome and victim demographics as predictors. This uses the `glm` function with the family specified to account for the non-Gaussian outcome distribution.

```{r, eval = FALSE}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
```

The table below summaries the coefficients from the model fit; because logistic model estimates are log odds ratios, we include a step to compute odds ratios as well.

```{r, eval = FALSE}
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)
```

We can also compute fitted values; similarly to the estimates in the model summary, these are expressed as log odds and can be transformed to produce probabilities for each subject.

```{r, eval = FALSE}
baltimore_df %>% 
  modelr::add_predictions(fit_logistic) %>% 
  mutate(fitted_prob = boot::inv.logit(pred))
```

# Cross Validation

* Cross validation provides a way to compare the predictive performance of competing methods. 

## Simulate data

non-linear dataset:

```{r}
nonlin_df = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10*(x - 0.3)^2 + rnorm(100, 0, 0.3)
  )

nonlin_df %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()
```


## Cross validation -- "by hand"

Get training and testing datasets

```{r}
train_df = sample_n(nonlin_df,size = 80)
test_df = anti_join(nonlin_df, train_df, by = "id")
```

Fit three models

```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = gam(y ~ s(x), data = train_df)
wiggly_mod = gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```


* `mgcv::gam` package for non-linear models. The three models below have very different levels of complexity and arent' nested, so testing approaches for nested model don't apply.

```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = mgcv::gam(y ~ s(x), data = train_df)
wiggly_mod = mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```

To see what these models have done, plot the two `gam` fits

```{r}
train_df %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")

train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")   # this model works better

train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")  # this makes bad prediction because it is chasing too much

```

Can use `modelr::gather_predictions` function to add predictions for several models to a data frame and then "pivoting" 

```{r}
train_df %>% 
  gather_predictions(linear_mod, smooth_mod, wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red") +
  facet_grid(. ~ model)

# Linear model is too simple, never capture the true relationship between variables because of its simplicity. The standard `gam` fit is pretty good. The wiggly `gam` fit is too complex, chasing data points and will change a lot from one training dataset to the next. Smooth model should do best in term of accuracy. 
```

Look at prediction accuracy

```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```

*The `modelr` has other outcome measures - RMSE is the most common, but median absolute deviation is pretty common as well. The RMSEs are suggestive that both nonlinear models work better than the linear model, and that the smooth fit is better than the wiggly fit. 


## CV using `modelr` 

* `crossv_mc` performs the training/testing split multiple times.

```{r}
cv_df = 
  crossv_mc(nonlin_df, 100)
```

What is happening here...

```{r}
cv_df %>%  pull(train) %>% .[[1]] %>%  as_tibble()
cv_df %>%  pull(test) %>% .[[1]] %>%  as_tibble()
```


```{r}
cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

I now have many training and testing datasets, and I’d like to fit my candidate models above and assess prediction accuracy as I did for the single training / testing split. To do this, I’ll fit models and obtain RMSEs using **mutate** + **map** & **map2**.


```{r}
cv_df = 
  cv_df %>% 
  mutate(
    linear_mod  = map(train, ~lm(y ~ x, data = .x)),
    smooth_mod  = map(train, ~mgcv::gam(y ~ s(x), data = .x)),
    wiggly_mod  = map(train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))
    ) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y)),
    rmse_wiggly = map2_dbl(wiggly_mod, test, ~rmse(model = .x, data = .y))
    )
```

What do these results say about model choices?

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```


Compute averages...

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  group_by(model) %>% 
  summarize(avg_rmse = mean(rmse))
```


## Real dataset

* A cross-sectional study of Nepalese children was carried out to understand the relationships between various measures of growth, including weight and arm circumference.

#### import and look at the data

```{r}
child_growth = 
  read_csv("data/nepalese_children.csv")

child_growth %>% 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .5)

# The plots suggests some non-linearity, especially at the low end of the weight distribution. We’ll try three models: a linear fit; a piecewise linear fit; and a smooth fit using gam. For the piecewise linear fit, we need to add a “change point term” to our dataframe.
```


```{r}
child_growth =
  child_growth %>% 
  mutate(weight_cp = (weight > 7) * (weight - 7))
```


The code chunk below fits each of the candidate models to the full dataset. The piecewise linear model is nested in the linear model and could be assessed using statistical significance, but the smooth model is not nested in anything else. (Also, comparing a piecewise model with a changepoint at 7 to a piecewise model with a changepoint at 8 would be a non-nested comparison…)

```{r}
linear_mod = lm(armc ~ weight, data = child_growth)
pwlin_mod = lm(armc ~ weight + weight_cp, data = child_growth)  #piecewise linear model
smooth_mod = gam(armc ~ s(weight), data = child_growth)
```

Plot the three models to get intuition for goodness of fit

```{r}
child_growth %>% 
  gather_predictions(linear_mod, pwlin_mod, smooth_mod) %>% 
  ggplot(aes(x = weight, y = armc)) +
  geom_point(alpha = .3) +
  geom_line(aes(y = pred), color = "red") +
  facet_grid(. ~ model)

#not clear which model is best => check prediction errors using the same process as before. 
```


Try to understand model fit using CV.

```{r}
cv_df = 
  crossv_mc(child_growth, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

Next I’ll use mutate + map & map2 to fit models to training data and obtain corresponding RMSEs for the testing data.

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    linear_mod  = map(train, ~lm(armc ~ weight, data = .x)),
    pwl_mod     = map(train, ~lm(armc ~ weight + weight_cp, data = .x)),
    smooth_mod  = map(train, ~gam(armc ~ s(weight), data = as_tibble(.x)))
    ) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, ~rmse(model = .x, data = .y)),
    rmse_pwl    = map2_dbl(pwl_mod, test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y))
    )
```

Violin plots of RMSEs

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()

# I’d probably go with the piecewise linear model – the non-linearity is clear enough that it should be accounted for, and the differences between the piecewise and gam fits are small enough that the easy interpretation of the piecewise model “wins”
```


# Bootstrapping

* Assumptions:
   + Sample means follow a known distribution
   + Regression coefficients follow a known distribution
   + Odds ratios follow a known distribution
   
* Process:
   + Write a function to: draw a sample with replacement, analyze the sample, return object of interest
   + Repeat this process many times
   
## Simulate data
   
```{r}
n_samp = 250

sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

sim_df_nonconst = 
  sim_df_const %>% 
  mutate(
    error = error * 0.75 * x,
    y = 2 + 3 * x + error
  )
```

Plot the datasets

```{r}
sim_df_const %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm")

sim_df_nonconst %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm")
```

fit a regression line

```{r}
lm( y ~ x, data = sim_df_const) %>% broom::tidy()

lm(y ~ x, data = sim_df_nonconst) %>% broom::tidy()

```

## Draw one bootstrap sample

```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE) %>% 
    arrange(x)
  
}
```

Check if this works...

```{r}
boot_sample(sim_df_nonconst) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha  = .3) + 
  geom_smooth(method = "lm") +
  ylim(-5, 16)
```

```{r}
boot_sample(sim_df_nonconst) %>% 
  lm(y ~ x, data = .) %>% 
  broom::tidy()
```

## Many samples and analysis

```{r}
boot_straps = 
  tibble(
    strap_number = 1:1000,
    strap_sample = rerun(1000, boot_sample(sim_df_nonconst))
    )

```

Can I run my analysis on these...?

```{r}
boot_results = 
  boot_straps %>% 
  mutate(
    models = map(.x = strap_sample, ~lm (y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)

```

What do I have now?

```{r}
boot_results %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est = sd(estimate)
  )
```

Look at the distributions

```{r}
boot_results %>% 
  filter(term == "x") %>% 
  ggplot(aes(x = estimate)) +
  geom_density() 
```

Construct a bootstrap CI

```{r}
boot_results %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.975)
  )
```

## Boostrap using modelr

```{r}
sim_df_nonconst %>% 
  bootstrap(1000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm (y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est = sd(estimate)
  )

  
```

## Revisit nyc airbnb

Reload the nyc airbnb dataset

```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") %>% 
  select(price, stars, borough, neighborhood, room_type)
```

```{r}
nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price)) +
  geom_point()
```


```{r}
airbnb_boot_results =
  nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  drop_na(stars) %>% 
  bootstrap(100, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm (price ~ stars, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)

airbnb_boot_results %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est = sd(estimate)
  )

```

Compare this to `lm`

```{r}
nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  drop_na(stars) %>% 
  lm(price ~ stars, data = .) %>% 
  broom::tidy()
```


```{r}
airbnb_boot_results %>% 
  filter(term == "stars") %>% 
  ggplot(aes(x = estimate)) +
  geom_density()
```

# Live session - HW6

## Problem 1

Look at some variables: age, race and sex

```{r error = TRUE}
homicide_df =
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```


Start with one city

```{r error = TRUE}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")

glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = bionomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```


Try this across cities

```{r error = TRUE}
models_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(.x = data, glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial)),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI"))

```


Make a plot comparing OR between male and female homicide victim along with CI

```{r error = TRUE}
models_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


## Problem 2

some variables: birthweight, sex, bwt, gaweeks, momage, parity, smoken

build the model: by looking thru these variables and come up with reasons why you put them in. or go all out and variable selection?

Compare in term of prediction accuracy and prediction errors. 

```{r error = TRUE}
# Find some residuals

baby_df = 
  read_csv("data/birthweight.csv") 

# Fit a model

model_fit = lm(bwt ~ gaweeks, data = baby_df) #can make a scatterplot with y = bwt,x = gastational weeks

# add residuals
baby_df %>% 
  modelr::add_residuals(model_fit) %>% 
  ggplot(aes(x = resid)) +
  geom_density() #residuals are not heavily skewed. 

baby_df %>% 
  modelr::add_residuals(model_fit) %>% 
  ggplot(aes(x = gaweeks, y = resid)) +
  geom_point() #everything centered around 0. 
```


## Problem 3

use bootstraps to obtain distribution for parameters interested in. 
Fit a linear model of tmax and tmin. Proporiton of r squared value (broom::glance) and log of (bo + b1) (broom::tidy)
