---
title: "Linear Models"
author: "Linh Tran"
date: "11/21/2020"
output: 
    html_document:
      toc: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d


set.seed(1)

```

# Linear models

Load and cleans the `airbnb` data.

```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") %>% 
  select(price, stars, borough, neighborhood, room_type)

```

## Model fitting

Scatterplot:

```{r}
#price = continuous variable, stars = numeric predictor, borough = categorical predictor

nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price, color = borough)) +
  geom_point()
```

Fit a model consider price as an outcome that may depend on rating and borough

```{r}
fit = lm(price ~ stars + borough, data = nyc_airbnb)
fit
```

You can also specify an intercept-only model (outcome ~ 1), a model with no intercept (outcome ~ 0 + ...), and a model using all available predictors (outcome ~ .)

## Tidying output

The output of a lm is an object of class lm – a very specific list that isn’t a dataframe but that can be manipulated using other functions. Some common functions for interacting with lm fits are below, although we omit the output:

```{r, include = FALSE}
fit
summary(fit)  #produces an object of class summary.lm, which is a list
summary(fit)$coef
coef(fit) #produce a vector of coef values
fitted.values(fit)  #vector of fitted values
residuals(fit)

broom::glance(fit)  #gives me some useful numbers
```

The `broom` package has functions for obtaining a quick summary of the model and for cleaning up the coefficient table. Both of these functions produce df.

```{r}
broom::tidy(fit) %>% #structured as a df
  select(-std.error, -statistic) %>% 
  mutate(
    term = str_replace(term, "borough", "Borough: ")
  ) %>% 
  knitr::kable(digits = 3)
```

## Be in control of factors

R treat categorical (factor) covariates as such: indicator variables are created for each non-reference category and included in model, and the factor level is treated as the reference.

```{r}
nyc_airbnb =
  nyc_airbnb %>% 
  mutate(
    borough = fct_infreq(borough),
    room_type = fct_infreq(room_type)
  )

nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price, color = borough)) +
  geom_point()
```

Refit my model: now Manhattan is the reference 

```{r}
fit = lm(price ~ stars + borough, data = nyc_airbnb)

broom::tidy(fit)
```

## Diagnostics

Regression diagnostics can identify issues in model fit, especially related to certian failures in model assumptions. The `modelr` package can be used to add residuals and fitted values to a dataframe.

```{r}
modelr::add_residuals(data = nyc_airbnb, model = fit)
# First one is $9.47 higher than what my model predicts.
```

Look at distribution of residuals:

```{r}
modelr::add_residuals(data = nyc_airbnb, model = fit) %>% 
  ggplot(aes(x = borough, y = resid)) +
  geom_violin() +
  ylim(-500, 1500)

#Distributions of residuals are skewed. 

nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = stars, y = resid)) +
  geom_point() +
  facet_wrap(. ~ borough)

#Distributions of residual spread out as star counts increases. 
```

For exclusion of outliers: transform the price variable (e.g. using a log transformation), or fitting a model that is robust to outliers. E.g: a combination of median regression, which is less sensitive to outliers than OLS, maybe bootstrapping for inference. 

## Hypothesis testing

* Model summaries include results of t-tests for single coefficients, and are the standard way of assessing statistical significance.
* Testing multiple coef is somewhat more complicated.  A useful approach is to use nested models, meaning that the terms in a simple “null” model are a subset of the terms in a more complex “alternative” model.

This does t-test by default

```{r}
fit %>% 
  broom::tidy()
```

Assess the significance of a categorical predictor with more than two levels 

```{r}
fit_null = lm(price ~ stars, data = nyc_airbnb)
fit_alt = lm(price~stars + borough, data = nyc_airbnb)

anova(fit_null, fit_alt) %>% 
  broom::tidy()
```


## Nest data, fit models

* Fitting models to datasets nested within variables, meaning, that we'll use `nest` to create a list column containing datasets and fit separate models to each. 

In the airbnb data, we might think that star ratings and room type affects price differently in each borough. One way to allow this kind of effect modification is through interaction terms:

```{r}
#interaction

fit = lm(price ~ stars * borough + room_type * borough, data = nyc_airbnb)

broom::tidy(fit)

# another way
nyc_airbnb %>% 
  lm(price ~ stars * borough + room_type * borough, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

Alternatively, we can nest within boroughs and fit borough-specific models associating price with rating and room type:

```{r}
nyc_airbnb %>% 
  nest(data = -borough) %>% 
  mutate(
    models = map(.x = data, ~lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  filter(term != "(Intercept)") %>% 
  select(borough, term, estimate) %>% 
  pivot_wider(
    names_from = borough,
    values_from = estimate
  )
  
```

**Fitting models to nested datasets is a way of performing stratified analyses. These have a tradeoff: stratified models make it easy to interpret covariate effects in each stratum, but don’t provide a mechanism for assessing the significance of differences across strata.**

An even more extreme example is the assessment of neighborhood effects in Manhattan. The code chunk below fits neighborhood-specific models and shows neighborhood-specific estimates for the coefficients related to room type:

```{r}
nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  nest(data = -neighborhood) %>% 
  mutate(
    models = map(.x = data, ~lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(-data, -models) %>% 
  unnest(results)  %>% 
  filter(str_detect(term, "room_type")) %>% 
  ggplot(aes(x = neighborhood, y = estimate)) +
  geom_point() +
  facet_wrap(. ~ term) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

#There is, generally speaking, a reduction in room price for a private room or a shared room compared to an entire apartment, but this varies quite a bit across neighborhoods.
```

Code to fit a mixed model with neighborhood-level random intercepts and random slopes for each room type

```{r}
manhattan_airbnb =
  nyc_airbnb %>% 
  filter(borough == "Manhattan")

manhattan_airbnb %>% 
  lme4::lmer(price ~ stars + room_type + (1 + room_type | neighborhood), data = .) %>% 
  broom.mixed::tidy()
```

## Binary outcomes

Logistic regression is useful for binary outcomes

```{r, eval = FALSE}
baltimore_df = 
  read_csv("data/homicide-data.csv") %>% 
  filter(city == "Baltimore") %>% 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) %>% 
  select(resolved, victim_age, victim_race, victim_sex)
```

Using these data, we can fit a logistic regression for the binary “resolved” outcome and victim demographics as predictors. This uses the `glm` function with the family specified to account for the non-Gaussian outcome distribution.

```{r, eval = FALSE}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
```

The table below summaries the coefficients from the model fit; because logistic model estimates are log odds ratios, we include a step to compute odds ratios as well.

```{r, eval = FALSE}
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)
```

We can also compute fitted values; similarly to the estimates in the model summary, these are expressed as log odds and can be transformed to produce probabilities for each subject.

```{r, eval = FALSE}
baltimore_df %>% 
  modelr::add_predictions(fit_logistic) %>% 
  mutate(fitted_prob = boot::inv.logit(pred))
```

# Cross Validation

## Simulate data

non-linear dataset:

```{r}
nonlin_df = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10*(x - 0.3)^2 + rnorm(100, 0, 0.3)
  )

nonlin_df %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()
```


## Cross validation -- "by hand"

Get training and testing datasets

```{r}
train_df = sample_n(nonlin_df,size = 80)
test_df = anti_join(nonlin_df, train_df, by = "id")
```

Fit three models

```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = gam(y ~ s(x), data = train_df)
wiggly_mod = gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```

Can I see what I just did?

```{r}
train_df %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")

train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")   # this model works better

train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")  # this makes bad prediction because it is chasing too much

```

Smooth model should do best in term of accuracy. 

```{r}
train_df %>% 
  gather_predictions(linear_mod, smooth_mod, wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red") +
  facet_grid(. ~ model)
```

Look at prediction accuracy

```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```


## CV using `modelr` 





# Bootstrapping
